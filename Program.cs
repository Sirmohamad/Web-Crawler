using System;
using System.Linq;
using System.Threading.Tasks;
using WebCrawler.Models;
using WebCrawler.Services;

namespace WebCrawler;

class Program
{
    static async Task Main(string[] args)
    {
        Console.OutputEncoding = System.Text.Encoding.UTF8;
        Console.InputEncoding = System.Text.Encoding.UTF8;

        Console.WriteLine("===== Web Crawler with .NET =====\n");
        Console.WriteLine("âœ¨ All settings are configured automatically!");
        Console.WriteLine("ðŸ“ Enter URL (or press Enter to use default)\n");

        // Get input link
        Console.Write("ðŸŒ Enter input link: ");
        string startUrl = Console.ReadLine()?.Trim() ?? "";

        if (string.IsNullOrEmpty(startUrl))
        {
            startUrl = "https://example.com";
            Console.WriteLine($"âœ“ Using default URL: {startUrl}");
        }

        Console.WriteLine("\nâš™ï¸  Get optional settings (or press Enter to use defaults):\n");

        Console.Write("ðŸ“ Limit to a specific section by ID? (optional): ");
        var sectionId = Console.ReadLine()?.Trim();
        if (string.IsNullOrEmpty(sectionId))
            sectionId = null;

        Console.Write("ðŸ“Š Maximum crawling depth (default: 10): ");
        var maxDepthInput = Console.ReadLine()?.Trim();
        var maxDepth = int.TryParse(maxDepthInput, out var depth) ? depth : 10;

        Console.WriteLine("\nâœ¨ Starting crawling with the following settings:");
        Console.WriteLine($"   ðŸ“Œ URL: {startUrl}");
        if (!string.IsNullOrEmpty(sectionId))
            Console.WriteLine($"   ðŸ“Œ Section ID: #{sectionId}");
        Console.WriteLine($"   ðŸ“Œ Max Depth: {maxDepth}");
        Console.WriteLine($"   ðŸ“Œ Download PDF: âœ…");
        Console.WriteLine($"   ðŸ“Œ Download Word: âœ…");
        Console.WriteLine($"   ðŸ“Œ Download Excel: âœ…");
        Console.WriteLine($"   ðŸ“Œ Download PowerPoint: âœ…");
        Console.WriteLine($"   ðŸ“Œ Download Images: âŒ (disabled)");
        Console.WriteLine($"   ðŸ“Œ Download Videos: âŒ (disabled)");
        Console.WriteLine($"   ðŸ“Œ Download Audio: âŒ (disabled)");
        Console.WriteLine($"   ðŸ“Œ Storage Folder: downloads/");
        Console.WriteLine();

        // Create config with default settings
        var config = new CrawlerConfig
        {
            StartUrl = startUrl,
            SectionId = sectionId,
            MaxDepth = maxDepth,
            DelayMs = 500,  // Half a second delay between requests
            // Use static ID list
            TargetElementIds = TargetElementIds.IsEnabled ? TargetElementIds.Ids : null
            // All selectors are configured by default
        };

        // Create and start crawling
        var crawler = new WebCrawlerService(config);
        
        crawler.UrlVisited += (sender, url) =>
        {
            Console.WriteLine($"âœ“ {url}");
        };

        crawler.ProgressUpdated += (sender, e) =>
        {
            Console.WriteLine($"  â†’ Status: {e.ProcessedLinks}/{e.TotalLinks} links processed at depth {e.Depth}");
        };

        var rootNode = await crawler.StartCrawlingAsync();

        // Display crawled tree
        Console.WriteLine("\n===== Crawled Tree =====");
        PrintNodeTree(rootNode, 0);

        Console.WriteLine("\n===== Final Report =====");
        Console.WriteLine($"ðŸ“„ Total pages: {CountTotalNodes(rootNode)}");
        Console.WriteLine($"ðŸ“Š Tree depth: {GetMaxDepth(rootNode)}");
        Console.WriteLine($"ðŸ’¾ Files in folder: downloads/");

        Console.WriteLine("\nâœ… Crawling completed successfully!");
        Console.WriteLine("Press Enter to exit...");
        Console.ReadLine();
    }

    static void PrintNodeTree(PageNode node, int indent)
    {
        var indentStr = new string(' ', indent * 2);
        Console.WriteLine($"{indentStr}ðŸ“„ {node.Url}");
        
        foreach (var child in node.Children)
        {
            PrintNodeTree(child, indent + 1);
        }
    }

    static int CountTotalNodes(PageNode node)
    {
        return 1 + node.Children.Sum(child => CountTotalNodes(child));
    }

    static int GetMaxDepth(PageNode node)
    {
        if (!node.Children.Any())
            return node.Depth;
        
        return node.Children.Max(child => GetMaxDepth(child));
    }
}
